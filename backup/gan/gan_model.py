import os, math
import tensorflow as tf

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


class GAN:

    def __init__(self, _MAXLENGTH, interface):
        """
        Defines the structure and calculation model of a GAN
        """
        # Parameter
        self._MAXLENGTH = _MAXLENGTH
        self._MAXWORDLENGTH = int(_MAXLENGTH / 5)
        self.itf = interface
        #self.generator_input_length = int(_MAXLENGTH/3)
        self.generator_input_length = 10

        g_afterinput = 1/math.sqrt(self.generator_input_length)
        g_hidden_nodes = int(math.sqrt(self.generator_input_length * _MAXLENGTH))
        g_afterhidden = 1/math.sqrt(g_hidden_nodes)
        g_afteroutput = 1/math.sqrt(_MAXLENGTH)

        d_afterinput = g_afteroutput
        d_hidden_nodes = int(math.sqrt(_MAXLENGTH))
        d_afterhidden = 1/math.sqrt(d_hidden_nodes)

        """
        print("\nGenerator params: ")
        print("Input size:\t" + str(self.generator_input_length))
        print("After input:\t" + str(g_afterinput))
        print("Hidden Nodes:\t" + str(g_hidden_nodes))
        print("After Hidden:\t" + str(g_afterhidden))
        print("Output Nodes:\t" + str(_MAXLENGTH))
        print("After Output:\t" + str(g_afteroutput))

        print("\nDiscriminator params: ")
        print("Input size:\t" + str(_MAXLENGTH))
        print("After input:\t" + str(d_afterinput))
        print("Hidden Nodes:\t" + str(d_hidden_nodes))
        print("After Hidden:\t" + str(d_afterhidden))
        print("Output Nodes:\t" + str(1))
        """

        self.G_X = tf.placeholder(tf.float32, shape=[None, self.generator_input_length])
        G_W1 = tf.Variable(tf.random_uniform([self.generator_input_length, 16], -0.037, 0.037))
        G_B1 = tf.Variable(tf.zeros([16]))
        G_W2 = tf.Variable(tf.random_uniform([16, 50], -0.088, 0.088))
        G_B2 = tf.Variable(tf.zeros([50]))
        G_model_part1 = tf.nn.relu(tf.matmul(self.G_X, G_W1) + G_B1)
        self.G_model = tf.sigmoid(tf.matmul(G_model_part1, G_W2) + G_B2)

        # Discriminator model
        self.D_X = tf.placeholder(tf.float32, shape=[None, 50])
        D_W1 = tf.Variable(tf.random_uniform([50, 16], -0.18, 0.18))
        D_B1 = tf.Variable(tf.zeros([16]))
        D_W2 = tf.Variable(tf.random_uniform([16, 1], -0.088, 0.088))
        D_B2 = tf.Variable(tf.zeros(1))
        D_model_part1_X = tf.nn.relu(tf.matmul(self.D_X, D_W1) + D_B1)
        self.D_model_X = tf.sigmoid(tf.matmul(D_model_part1_X, D_W2) + D_B2)
        D_model_part1_G = tf.nn.relu(tf.matmul(self.G_model, D_W1) + D_B1)
        D_model_G = tf.sigmoid(tf.matmul(D_model_part1_G, D_W2) + D_B2)

        # Define loss functions
        D_loss = -tf.reduce_mean(tf.log(self.D_model_X) + tf.log(1. - D_model_G))
        G_loss = -tf.reduce_mean(tf.log(D_model_G))

        # Define train functions
        self.D_train = tf.train.AdamOptimizer().minimize(D_loss, var_list=[D_W1, D_W2, D_B1, D_B2])
        self.G_train = tf.train.AdamOptimizer().minimize(G_loss, var_list=[G_W1, G_W2, G_B1, G_B2])

        # Define logging data for discriminator
        tf.summary.scalar("discriminator loss", D_loss)
        self.summary_op_d = tf.summary.merge_all()

        # Define logging data for generator
        # tf.summary.scalar("generator loss", G_loss)
        # self.summary_op = tf.summary.merge_all()

        # Initialize Session
        self.sess = tf.Session()
        self.sess.run(tf.global_variables_initializer())

    def train(self, data):
        """
        Trains the model with given dataset (list of samples)
        """

        # Get random inputs for discriminator
        random_inputs = []
        for i in range(len(data)):
            random_inputs.append(self.itf.getRandomInput(self.generator_input_length))

        # Train discriminator
        #print(random_inputs)
        summary, placeholder = self.sess.run([self.summary_op_d, self.D_train], feed_dict={self.D_X: data, self.G_X: random_inputs})
        #self.sess.run(self.D_train, feed_dict={self.D_X: data, self.G_X: random_inputs})

        # Get random inputs for generator
        random_inputs = []
        for i in range(len(data)):
            random_inputs.append(self.itf.getRandomInput(self.generator_input_length))

        # train generator
        #summary, placeholder = self.sess.run([self.summary_op_g, self.G_train], feed_dict={self.G_X: random_inputs})
        self.sess.run(self.G_train, feed_dict={self.G_X: random_inputs})
        return summary


    def getSample(self):
        """
        Returns a sample generated by the Generator Net of the GAN
        """
        return self.sess.run(self.G_model, feed_dict={self.G_X: [self.itf.getRandomInput(self.generator_input_length)]})


    def evaluate(self):
        """
        Calculates the accuracy of the model and returns it
        """
        # Evaluate Generator
        pos = 0
        for i in range(1000):
            s = self.itf.calcstring(self.getSample()[0])
            if self.itf.match(s):
                pos += 1
        g_acc = (pos/1000)

        # Evaluate Discriminator
        pos = 0
        samples = []
        for i in range(10):
            s = self.itf.get_random_string()
            samples.append(self.itf.calcvector(s))
        answers = self.sess.run(self.D_model_X, feed_dict={self.D_X: samples})

        for i in range(10):
            #print("NN: " + str(answers[i]) + "\tCorrect: " + str(self.itf.match(self.itf.calcstring(samples[i]))))
            if (self.itf.match(self.itf.calcstring(samples[i])) == 1 and answers[i] > 0.5) or (self.itf.match(self.itf.calcstring(samples[i])) == 0 and answers[i] < 0.5):
                pos += 1
        d_acc = (pos/10)

        return (g_acc, d_acc)